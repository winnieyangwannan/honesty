from abc import ABC, abstractmethod
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, GenerationConfig
from transformers import GPTNeoXForCausalLM

from tqdm import tqdm
from torch import Tensor
from jaxtyping import Int, Float

from pipeline.utils.hook_utils import add_hooks

class ModelBase(ABC):
    def __init__(self, model_name_or_path: str, checkpoint=None):
        self.model_name_or_path = model_name_or_path
        self.checkpoint = checkpoint
        if "pythia" in model_name_or_path:
            self.model: GPTNeoXForCausalLM = self._load_model(model_name_or_path, checkpoint=checkpoint)
        else:
            self.model: AutoModelForCausalLM = self._load_model(model_name_or_path, checkpoint=checkpoint)
        self.tokenizer: AutoTokenizer = self._load_tokenizer(model_name_or_path, checkpoint=checkpoint)
        
        self.tokenize_instructions_fn = self._get_tokenize_instructions_fn()
        self.tokenize_statements_fn = self._get_tokenize_statements_fn()

        self.layer_norm = self._get_layer_norm()
        self.lm_head = self._get_lm_head()

        self.eoi_toks = self._get_eoi_toks() #eoi= end of instruction token?
        self.refusal_toks = self._get_refusal_toks()
        self.true_token_id = self._get_true_toks()
        self.false_token_id = self._get_false_toks()

        self.model_block_modules = self._get_model_block_modules()
        self.model_attn_modules = self._get_attn_modules()
        self.model_mlp_modules = self._get_mlp_modules()

    def del_model(self):
        if hasattr(self, 'model') and self.model is not None:
            del self.model

    @abstractmethod
    def _load_model(self, model_name_or_path: str) -> AutoModelForCausalLM:
        pass

    @abstractmethod
    def _load_tokenizer(self, model_name_or_path: str) -> AutoTokenizer:
        pass

    @abstractmethod
    def _get_tokenize_instructions_fn(self, system_type=None):
        pass

    @abstractmethod
    def _get_tokenize_statements_fn(self):
        pass

    @abstractmethod
    def _get_eoi_toks(self):
        pass

    @abstractmethod
    def _get_refusal_toks(self):
        pass

    @abstractmethod
    def _get_false_toks(self):
        pass

    @abstractmethod
    def _get_true_toks(self):
        pass

    @abstractmethod
    def _get_model_block_modules(self):
        pass

    @abstractmethod
    def _get_attn_modules(self):
        pass

    @abstractmethod
    def _get_mlp_modules(self):
        pass

    @abstractmethod
    def _get_orthogonalization_mod_fn(self, direction: Float[Tensor, "d_model"]):
        pass

    @abstractmethod
    def _get_act_add_mod_fn(self, direction: Float[Tensor, "d_model"], coeff: float, layer: int):
        pass

    @abstractmethod
    def _get_layer_norm(self):
        pass

    @abstractmethod
    def _get_lm_head(self):
        pass

    def generate_completions(self, dataset, fwd_pre_hooks=[], fwd_hooks=[], batch_size=8, max_new_tokens=64):
        generation_config = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)
        generation_config.pad_token_id = self.tokenizer.pad_token_id

        completions = []
        instructions = [x['instruction'] for x in dataset]
        categories = [x['category'] for x in dataset]

        for i in tqdm(range(0, len(dataset), batch_size)):
            tokenized_instructions = self.tokenize_instructions_fn(instructions=instructions[i:i + batch_size])

            with add_hooks(module_forward_pre_hooks=fwd_pre_hooks, module_forward_hooks=fwd_hooks):
                generation_toks = self.model.generate(
                    input_ids=tokenized_instructions.input_ids.to(self.model.device),
                    attention_mask=tokenized_instructions.attention_mask.to(self.model.device),
                    generation_config=generation_config,
                )

                generation_toks = generation_toks[:, tokenized_instructions.input_ids.shape[-1]:]

                for generation_idx, generation in enumerate(generation_toks):
                    completions.append({
                        'category': categories[i + generation_idx],
                        'prompt': instructions[i + generation_idx],
                        'response': self.tokenizer.decode(generation, skip_special_tokens=True).strip()
                    })

        return completions
